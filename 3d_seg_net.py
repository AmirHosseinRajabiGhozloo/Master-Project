# -*- coding: utf-8 -*-
"""3D_Seg_Net.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1FV5fsZTv8RI1n-vxgT9ojD0ts5_JJ7vW
"""

from google.colab import drive
drive.mount('/content/gdrive')

# 1. Import Required Modules

import os
import glob
import keras
import random
import numpy as np
import tensorflow as tf
from keras.layers import *
import keras.backend as k
from keras.models import *
from keras.optimizers import *
import matplotlib.pyplot as plt
from skimage.transform import resize
from skimage.io import imread, imshow, imsave, imread_collection
from keras.losses import categorical_crossentropy
from keras.callbacks import ModelCheckpoint, LearningRateScheduler, EarlyStopping

TRAIN_IMAGE_PATH = 'gdrive/My Drive/Berea_Sand_Texas/Image_Berea_UF_512_'
TRAIN_MASK_PATH = 'gdrive/My Drive/Berea_Sand_Texas/Mask_Berea_512_'

# 3. Initialize Images and Mask Size

IMG_HEIGHT, IMG_WIDTH, IMG_DEPTH, IMG_CHANNELS = 64, 64, 64, 1
input_shape = (IMG_HEIGHT, IMG_WIDTH, IMG_DEPTH, IMG_CHANNELS)

import glob
import cv2

Train_Input = [cv2.imread(file, cv2.IMREAD_GRAYSCALE) for file in sorted(glob.glob("gdrive/My Drive/Sand_data/Sand_data/train_image/*.png"))]
Train_Mask = [cv2.imread(file, cv2.IMREAD_GRAYSCALE) for file in sorted(glob.glob("gdrive/My Drive/Sand_data/Sand_data/train_label/*.png"))]

Train_Input = np.array(Train_Input)
Train_Mask = np.array(Train_Mask)

Train_Mask = cv2.normalize(Train_Mask, None, alpha=1,beta=0, norm_type=cv2.NORM_MINMAX)

Train_Input_3D = np.stack(Train_Input, axis=2)
Train_Mask_3D = np.stack(Train_Mask, axis=2)

Train_Mask_3D.shape

!pip install patchify

from patchify import *

Train_Input_3D_Patches = patchify(Train_Input_3D,(64,64,64), step=64)
Train_Mask_3D_Patches = patchify(Train_Mask_3D,(64,64,64), step=64)

Train_Mask_3D_Patches.shape

Train_Input_3D_Patche = np.reshape(Train_Input_3D_Patches, (-1, Train_Input_3D_Patches.shape[3], Train_Input_3D_Patches.shape[4], Train_Input_3D_Patches.shape[5]))
Train_Mask_3D_Patche = np.reshape(Train_Mask_3D_Patches, (-1, Train_Input_3D_Patches.shape[3], Train_Input_3D_Patches.shape[4], Train_Input_3D_Patches.shape[5]))

Train_Input_3D_Patche.shape

def iou_coef(y_true, y_pred, smooth=1):
  intersection = k.sum(k.abs(y_true * y_pred), axis=[1,2,3])
  union = k.sum(y_true,[1,2,3])+k.sum(y_pred,[1,2,3])-intersection
  iou = k.mean((intersection + smooth) / (union + smooth), axis=0)
  return iou

class MaxPoolingWithArgmax3D(Layer):
    def __init__(self, pool_size=(2, 2, 2), strides=(2, 2, 2), padding="same", **kwargs):
        super(MaxPoolingWithArgmax3D, self).__init__(**kwargs)
        self.padding = padding
        self.pool_size = pool_size
        self.strides = strides

    def call(self, inputs, **kwargs):
        padding = self.padding
        pool_size = self.pool_size
        strides = self.strides
        if k.backend() == "tensorflow":
            ksize = [1, pool_size[0], pool_size[1], pool_size[2], 1]
            padding = padding.upper()
            strides = [1, strides[0], strides[1], strides[2], 1]
            output, argmax = k.tf.nn.max_pool_with_argmax(
                inputs, ksize=ksize, strides=strides, padding=padding
            )
        else:
            errmsg = "{} backend is not supported for layer {}".format(
                k.backend(), type(self).__name__
            )
            raise NotImplementedError(errmsg)
        argmax = k.cast(argmax, k.floatx())
        return [output, argmax]

    def compute_output_shape(self, input_shape):
        ratio = (1, 2, 2, 2, 1)
        output_shape = [
            dim // ratio[idx] if dim is not None else None
            for idx, dim in enumerate(input_shape)
        ]
        output_shape = tuple(output_shape)
        return [output_shape, output_shape, output_shape]

    def compute_mask(self, inputs, mask=None):
        return 3 * [None]


class MaxUnpooling3D(Layer):
    def __init__(self, size=(2, 2, 2), **kwargs):
        super(MaxUnpooling3D, self).__init__(**kwargs)
        self.size = size

    def call(self, inputs, output_shape=None):
        updates, mask = inputs[0], inputs[1], inputs[2]
        with k.tf.compat.v1.variable_scope(self.name):
            mask = k.cast(mask, "int32")
            input_shape = k.tf.shape(updates, out_type="int32")
            #  calculation new shape
            if output_shape is None:
                output_shape = (
                    input_shape[0],
                    input_shape[1] * self.size[0],
                    input_shape[2] * self.size[1],
                    input_shape[3] * self.size[2],
                    input_shape[4],
                )
            self.output_shape1 = output_shape

            # calculation indices for batch, height, width and feature maps
            one_like_mask = k.ones_like(mask, dtype="int32")
            batch_shape = k.concatenate([[input_shape[0]], [1], [1], [1], [1]], axis=0)
            batch_range = k.reshape(
                k.tf.range(output_shape[0], dtype="int32"), shape=batch_shape
            )
            b = one_like_mask * batch_range
            y = mask // (output_shape[2] * output_shape[3])
            x = (mask // output_shape[3]) % output_shape[2]
            z = mask // (output_shape[3] * output_shape[4])
            feature_range = k.tf.range(output_shape[4], dtype="int32")
            f = one_like_mask * feature_range

            # transpose indices & reshape update values to one dimension
            updates_size = k.tf.size(updates)
            indices = k.transpose(k.reshape(k.stack([b, y, x, z, f]), [5, updates_size]))
            values = k.reshape(updates, [updates_size])
            ret = k.tf.scatter_nd(indices, values, output_shape)
            return ret

    def compute_output_shape(self, input_shape):
        mask_shape = input_shape[1]
        return (
            mask_shape[0],
            mask_shape[1] * self.size[0],
            mask_shape[2] * self.size[1],
            mask_shape[3] * self.size[2],
            mask_shape[4],
        )

def Seg_Net_3D_Segmentation(input_size=(IMG_HEIGHT, IMG_WIDTH, IMG_DEPTH, IMG_CHANNELS)):

    inputs = Input(input_size)
    n = Lambda(lambda x:x/255)(inputs)


    c1 = Conv3D(16, (3,3,3), activation='relu', padding='same')(n)
    #c1 = Dropout(0.1)(c1)
    c1 = Conv3D(16, (3,3,3), activation='relu', padding='same')(c1)


    #p1 = MaxPooling2D((2,2))(c1)
    p1, mask_1 = MaxPoolingWithArgmax3D((2,2,2))(c1)


    c2 = Conv3D(32, (3,3,3), activation='relu', padding='same')(p1)
    #c2 = Dropout(0.1)(c2)
    c2 = Conv3D(32, (3,3,3), activation='relu', padding='same')(c2)


    #p2 = MaxPooling2D((2,2))(c2)
    p2, mask_2 = MaxPoolingWithArgmax3D((2,2))(c2)


    c3 = Conv3D(64, (3,3,3), activation='relu', padding='same')(p2)
    #c3 = Dropout(0.2)(c3)
    c3 = Conv3D(64, (3,3,3), activation='relu', padding='same')(c3)
    #c3 = Dropout(0.2)(c3)
    c3 = Conv3D(64, (3,3,3), activation='relu', padding='same')(c3)


    #p3 = MaxPooling2D((2,2))(c3)
    p3, mask_3 = MaxPoolingWithArgmax3D((2,2,2))(c3)


    c4 = Conv3D(128, (3,3,3), activation='relu', padding='same')(p3)
    #c4 = Dropout(0.2)(c4)
    c4 = Conv3D(128, (3,3,3), activation='relu', padding='same')(c4)
    #c4 = Dropout(0.2)(c4)
    c4 = Conv3D(128, (3,3,3), activation='relu', padding='same')(c4)

    #p4 = MaxPooling2D((2,2))(c4)
    p4, mask_4 = MaxPoolingWithArgmax3D((2,2,2))(c4)


    c5 = Conv3D(256, (3,3,3), activation='relu', padding='same')(p4)
    #c5 = Dropout(0.2)(c5)
    c5 = Conv3D(256, (3,3,3), activation='relu', padding='same')(c5)
    #c5 = Dropout(0.2)(c5)
    c5 = Conv3D(256, (3,3,3), activation='relu', padding='same')(c5)


    p5, mask_5 = MaxPoolingWithArgmax3D((2,2,2))(c5)


    #u6 = Conv2DTranspose(128, (2,2), strides=(2,2), padding='same')(c5)
    #u6 = concatenate([u6, c4])
    up1 = MaxUnpooling3D((2,2,2))([p5, mask_5])


    c6 = Conv3D(256, (3,3,3), activation='relu', padding='same')(up1)
    #c6 = Dropout(0.2)(c6)
    c6 = Conv3D(256, (3,3,3), activation='relu', padding='same')(c6)
    #c6 = Dropout(0.2)(c6)
    c6 = Conv3D(128, (3,3,3), activation='relu', padding='same')(c6)


    #u7 = Conv2DTranspose(64, (2,2), strides=(2,2), padding='same')(c6)
    #u7 = concatenate([u7, c3])
    up2 = MaxUnpooling3D((2,2,2))([c6, mask_4])


    c7 = Conv3D(128, (3,3,3), activation='relu', padding='same')(up2)
    #c7 = Dropout(0.2)(c7)
    c7 = Conv3D(128, (3,3,3), activation='relu', padding='same')(c7)
    #c7 = Dropout(0.2)(c7)
    c7 = Conv3D(64, (3,3,3), activation='relu', padding='same')(c7)


    #u8 = Conv2DTranspose(32, (2,2), strides=(2,2), padding='same')(c7)
    #u8 = concatenate([u8, c2])
    up3 = MaxUnpooling3D((2,2,2))([c7, mask_3])


    c8 = Conv3D(64, (3,3,3), activation='relu', padding='same')(up3)
    #c8 = Dropout(0.1)(c8)
    c8 = Conv3D(64, (3,3,3), activation='relu', padding='same')(c8)
    #c8 = Dropout(0.1)(c8)
    c8 = Conv3D(32, (3,3,3), activation='relu', padding='same')(c8)


    #u9 = Conv2DTranspose(16, (2,2), strides=(2,2), padding='same')(c8)
    #u9 = concatenate([u9, c1], axis = 3)
    up4 = MaxUnpooling3D((2,2,2))([c8, mask_2])


    c9 = Conv3D(32, (3,3,3), activation='relu', padding='same')(up4)
    #c9 = Dropout(0.1)(c9)
    c9 = Conv3D(16, (3,3,3), activation='relu', padding='same')(c9)


    up5 = MaxUnpooling3D((2,2,2))([c9, mask_1])


    c10 = Conv3D(16, (3,3,3), activation='relu', padding='same')(up5)


    outputs = Conv3D(1,(1,1,1), activation='sigmoid')(c10)

    model = Model(inputs=[inputs], outputs=[outputs])
    model.compile(optimizer='adam',loss = ['binary_crossentropy'], metrics=[iou_coef])
    model.summary()
    return model

model = Seg_Net_3D_Segmentation()